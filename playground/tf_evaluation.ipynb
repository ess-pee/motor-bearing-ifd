{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c383b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from preprocess import preprocess\n",
    "\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hehe funny number\n",
    "np.random.seed(69)\n",
    "\n",
    "print(np.random.rand(1,1))\n",
    "\n",
    "# I'm not a patient man, if training saturated we can stop early\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# choose the dataset you want to use, cwru, mfd or tri\n",
    "DATASET = 'mfd'\n",
    "PERCENT_SAMPLE = 0.01 # choose the percentage of the dataset you want to use for training, 0.05 is 5%\n",
    "\n",
    "xtrain, ytrain, xtest, ytest, classes, window_size = preprocess(DATASET)\n",
    "\n",
    "# it wouldn't be transfer learning if I didn't sample the data into a smaller dataset so my experiment has some validity\n",
    "indices = np.random.choice(xtrain.shape[0], size=int(len(xtrain)*PERCENT_SAMPLE), replace=False)\n",
    "xtrain = xtrain[indices]\n",
    "ytrain = ytrain[indices]\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Input(shape=(window_size,1)))\n",
    "\n",
    "model.add(keras.layers.Conv1D(filters=128, kernel_size=9, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(keras.layers.Conv1D(filters=64, kernel_size=9, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(keras.layers.Conv1D(filters=32, kernel_size=9, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(keras.layers.Dense(len(classes)))\n",
    "model.add(keras.layers.Softmax())\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d465d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hehe funny number\n",
    "np.random.seed(69)\n",
    "\n",
    "print(np.random.rand(1,1))\n",
    "\n",
    "# I'm not a patient man, if training saturated we can stop early\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "DATASET = 'mfd' # choose the dataset you want to use, cwru, mfd or tri\n",
    "MODEL = 'cwru' # choose the model you want to use, cwru, mfd or tri\n",
    "PERCENT_SAMPLE = 0.01 # choose the percentage of the dataset you want to use for training, 0.05 is 5%\n",
    "\n",
    "pretrained_model = keras.models.load_model(f'models/{MODEL}_model.h5')\n",
    "\n",
    "transfer_model = keras.models.Sequential()\n",
    "transfer_model.add(keras.layers.Input(shape=(window_size,1)))\n",
    "\n",
    "for layer in pretrained_model.layers[:-7]:\n",
    "    layer.trainable = False\n",
    "    transfer_model.add(layer)\n",
    "\n",
    "for layer in transfer_model.layers[0:2]:\n",
    "    layer.trainable = True\n",
    "\n",
    "transfer_model.add(keras.layers.Flatten())\n",
    "\n",
    "transfer_model.add(keras.layers.Dense(32, activation='relu'))\n",
    "transfer_model.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "transfer_model.add(keras.layers.Dense(16, activation='relu'))\n",
    "transfer_model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "transfer_model.add(keras.layers.Dense(len(classes)))\n",
    "transfer_model.add(keras.layers.Softmax())\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "transfer_model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ce538",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcyd = []\n",
    "for i in range(20):\n",
    "    model.fit(xtrain, ytrain, epochs=100, batch_size=16, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "    _, validation_accuracy = model.evaluate(xtest, ytest, verbose=0)\n",
    "\n",
    "    transfer_model.fit(xtrain, ytrain, epochs=100, batch_size=16, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "    _, validation_accuracy_tf = transfer_model.evaluate(xtest, ytest, verbose=0)\n",
    "\n",
    "    wcyd.append((validation_accuracy*100, validation_accuracy_tf*100))\n",
    "    print(f'{i} training completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab0490",
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = [round(j-i,2) for i,j in wcyd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef4b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = sum(differences)/len(differences)\n",
    "mean_acc = sum([j for i,j in wcyd])/len(wcyd)\n",
    "\n",
    "print(f'Mean difference: {mean_diff:.2f}')\n",
    "print(f'Mean accuracy: {mean_acc:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
